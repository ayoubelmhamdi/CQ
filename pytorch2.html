<!DOCTYPE HTML>
<html lang="fr" class="sidebar-visible no-js navy">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>pyTorch 2 - Contrôle qualité en radiothérapie</title>


        <!-- Custom HTML head -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
        
        <link rel="stylesheet" href="css/tavianator.css">
        
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.js" integrity="sha384-YNHdsYkH6gMx9y3mRkmcJ2mFUjTd0qNQQvY9VYZgQd7DcN7env35GzlmFaZ23JGp" crossorigin="anonymous"></script>
        
        <script src="tavianator.js"></script>
        
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-EBDQXGZ6R1"></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-EBDQXGZ6R1');
        </script>

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="./theme/tabbed-code-blocks.css">

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "navy";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('navy')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> INTRODUCTION</a></li><li class="chapter-item expanded "><a href="NEMA2007_gamma_camerad.html"><strong aria-hidden="true">2.</strong> (EN) NEMA 2007 gamma camera</a></li><li class="chapter-item expanded "><a href="test.html"><strong aria-hidden="true">3.</strong> test</a></li><li class="chapter-item expanded "><a href="pytorch1.html"><strong aria-hidden="true">4.</strong> pyTorch 1</a></li><li class="chapter-item expanded "><a href="pytorch2.html" class="active"><strong aria-hidden="true">5.</strong> pyTorch 2</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Contrôle qualité en radiothérapie</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/ayoubelmhamdi/CQ" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/ayoubelmhamdi/CQ/tree/master/src/pytorch2.md" title="Suggest an edit" aria-label="Suggest an edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
<h1 id="learning-from-images-to-detect-lung-cancer-using-pytorch"><a class="header" href="#learning-from-images-to-detect-lung-cancer-using-pytorch">Learning from images to detect lung cancer using PyTorch</a></h1>
<p>The project is to create a detector for lung cancer using ct scans.</p>
<h1 id="xxx"><a class="header" href="#xxx">XXX</a></h1>
<h3 id="introduction"><a class="header" href="#introduction">introduction</a></h3>
<p>explores a single use case of detecting lung cancer in depth,The approach involves starting with the basic building blocks, and building out a more complete project using PyTorch.</p>
<ul>
<li>
<p>the project, environment, and data that will be used are converted into a PyTorch dataset,</p>
</li>
<li>
<p>a classification model is introduced and various solutions are implemented to problems preventing the model from training well.</p>
</li>
<li>
<p>a segmentation model is created that produces a heatmap rather than a single classification.</p>
</li>
<li>
<p>The segmentation and classification models are combined to perform a final diagnosis of lung cancer.</p>
</li>
</ul>
<p>The chapter also highlights the importance of breaking down complex problems into smaller ones, exploring the constraints of an intricate deep learning problem, and downloading the training data. The reader will learn about data formats, data sources, and how to explore the constraints that a problem domain places on the project.</p>
<p>The choosing to use the detection of malignant tumors in the lungs using only a CT scan of a patient's chest as input to illustrate how to overcome technical issues. Automatic detection of lung cancer is challenging, and even professional specialists face difficulty in identifying malignant tumors. Automating the process with deep learning will be more demanding and require a structured approach to succeeding.</p>
<p>Detecting lung cancer early is essential for increasing the patient's survival rate, but it's tough to do manually, especially on a large scale. Automating this process will provide readers with experience in dealing with difficult scenarios where solving problems is challenging.</p>
<p>The problem space of lung tumor detection is important because it is an active research area with promising results. However, it is also unsolved, which satisfies the authors' objective of using PyTorch to tackle state-of-the-art projects.</p>
<h1 id="preparing-for-a-large-scale-project"><a class="header" href="#preparing-for-a-large-scale-project">Preparing for a Large-Scale Project</a></h1>
<p>This project builds with a model consists of repeated convolutional layers followed by a resolution-reducing downsampling layer. However, the data will come in a <em>3D</em> format.</p>
<p>Because of there is no pre-built library to provide suitable training samples for the model. Therefore, we  will need to implement the data of manipulation. Additionally, and we need to understand that the project's approach will be more complicated to account for factors that could influence the process, such as limited data availability, finite computational resources, and limitations on our ability to design effective models.</p>
<p>we should access to a GPU with at least <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">8</span><span class="mspace nobreak"> </span><span class="mord mathnormal" style="margin-right:0.05017em;">GB</span></span></span></span> of RAM to achieve reasonable training speeds. If no GPU is available. Finally, we would require a minimum of <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">220</span><span class="mspace nobreak"> </span><span class="mord mathnormal" style="margin-right:0.05017em;">GB</span></span></span></span> of free disk space to store the raw training data, cached data, and trained models.</p>
<p>Instead of looking at the entire CT scan, the method of breaking the problem down into simpler tasks will be used. Before going into the details of how the problem will be broken down, information on CT scans and their format is given. It is explained that CT scans are essentially 3D X-rays comprising a three-dimensional array of single-channel data. Each voxel of a CT scan has a numeric value that approximately corresponds to the average mass density of the matter contained inside. CT scans can render the data in a variety of ways and show the third dimension of the data. CT scans are much harder to obtain than X-rays and cost a lot of money. The project focuses on creating a detector for lung cancer using CT scans, and the text emphasizes the importance of learning about the medical domain to get effective results.</p>
<h2 id="steps"><a class="header" href="#steps">steps</a></h2>
<p>The process of creating an end-to-end solution for detecting nodules in lung CT scans using PyTorch.</p>
<p>The process involves five main steps:</p>
<ul>
<li>loading the CT data.</li>
<li>segmenting the image to identify potential tumors</li>
<li>grouping interesting voxels to form candidates</li>
<li>classifying the nodules</li>
<li>and diagnosing the patient based on the malignancy of the identified nodules.</li>
</ul>
<p>The use of human-annotated data for training and the importance of learning from previous work in the field.</p>
<p>The development of intuition about the problem space and the importance of understanding the data and problem before jumping in. comes from going into detail about lung tumors and CT scans. the problem space is complex because the majority of CT scans are uninteresting, so identifying malignant tumors can be challenging.</p>
<p>where two networks worked together to create convincing and authentic artwork. Our approach to solving this problem focuses on optimizing individual parts of the project, as we don't want the segmentation and classification models to be trained together.</p>
<p>This single-task approach allows us to concentrate on each smaller skill and focus on specific areas of interest, rather than training the entire image at once. We will be classifying nodules in sequential transverse slices ,and then segmenting tumors.</p>
<p>then we will implement the end-to-end project using grouping and nodule analysis and diagnosis. We need to learn specifics about cancer and radiation oncology, such as what a nodule is, to understand the data better.</p>
<p>The cancers we are trying to detect will always appear as nodules. The smallest nodules can be just a few millimeters across, and malignant nodules display visual discrepancies from other nodules. The LUNA Grand Challenge provides an open dataset with high-quality labels of patient CT scans for researchers to use and perform work.</p>
<p>The goal is to encourage improvements to nodule detection and to test the efficacy of the detection methods against standardized criteria.
In this text, it is explained that CT scans can be messy, with differences in scanners and processing programs. The LUNA 2016 dataset will be used, which contains generally clean data. The data is around 60 GB compressed and takes up around 120 GB of space when uncompressed. Candidates.csv and annotations.csv files are needed alongside the data subsets.</p>
<p>The dataset comes in 10 subsets.</p>
<h1 id="cancerous-nodule-detection"><a class="header" href="#cancerous-nodule-detection">Cancerous nodule detection</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>The approach to detecting cancerous nodules in medical images. has five steps:</p>
<ul>
<li>data loading</li>
<li>segmentation</li>
<li>grouping</li>
<li>classification</li>
<li>nodule analysis and diagnosis.</li>
</ul>
<h2 id="data-processing"><a class="header" href="#data-processing">Data processing</a></h2>
<p>To process the data, it is necessary to convert raw data files into a format that is usable by PyTorch. The data is represented as a 3D array of intensity data, with around 32 million voxels, which is much larger than the nodules. To make the task more manageable, the model will focus on a relevant crop of the CT scan. There are various steps involved in processing the data, including understanding the data, mapping location information to array indexes, and converting the CT scan intensity into mass density. Identifying the key concepts of the project, such as nodules, is crucial.</p>
<h2 id="data-loading"><a class="header" href="#data-loading">Data loading</a></h2>
<p>The chapter focuses on the first step of the approach, which is data loading. The goal is to produce a training sample from raw CT scan data and a list of annotations. The process is described as transmuting the raw data into the stuff that the neural network will spin into gold.</p>
<p>The chapter covers the following topics:</p>
<ul>
<li>Loading and processing raw data files</li>
<li>Implementing a Python class to represent the data</li>
<li>Converting the data into a format usable by PyTorch</li>
<li>Visualizing the training and validation data</li>
</ul>
<p>Overall, the quality of the data used to train the model has a significant impact on the project's success.</p>
<h2 id="raw-ct-data-files"><a class="header" href="#raw-ct-data-files">Raw CT Data Files</a></h2>
<p>CT data comes in two files: a .mhd file of metadata header information and a .raw file of raw bytes. Each file name begins with the series UID. The CT class loads these files, processes the information to produce a 3D array, and transforms the patient coordinate system to the index, row, and column coordinates of each voxel in the array. Annotation data from LUNA with nodule coordinates and malignancy flags are also loaded, which are used to crop a small 3D slice of the CT data. The CT data, nodule status flag, series UID, and index of the sample are included in a sample tuple.</p>
<h2 id="parsing-lunas-annotation-data"><a class="header" href="#parsing-lunas-annotation-data">Parsing LUNA's Annotation Data</a></h2>
<p>The candidates.csv file contains information about all lumps that look like nodules, whether they are malignant, benign, or something else. We'll use this to build a list of candidates that can be split into training and validation datasets. The annotations.csv file contains information about some of the candidates that have been flagged as nodules, including the diameter. This information is useful for ensuring a representative range of nodule sizes in the training and validation data.</p>
<h1 id="training-and-validation-sets"><a class="header" href="#training-and-validation-sets">Training and validation sets</a></h1>
<p>For supervised learning tasks like classification, we need to split our data into training and validation sets. We want to ensure that both sets represent the real-world input data that we expect to see and handle normally. If either set is significantly different from our actual use cases, it's highly likely that our model will behave differently than we expect. This split helps us evaluate and improve the model's performance before we deploy it on production data.</p>
<p>In this context, we'll split our nodules' dataset by size and take every Nth one for our validation set. However, the location information provided in annotations.csv may not precisely line up with the coordinates in candidates.csv. So, we need to match them using the coordinates' respective measurements.</p>
<h1 id="loading-individual-ct-scans"><a class="header" href="#loading-individual-ct-scans">Loading individual CT scans</a></h1>
<p>We will take our CT data from a pile of bits on disk and turn it into a Python object from which we can extract 3D nodule density data. Our nodule annotation information acts like a map to the relevant parts of our raw data. Before we can follow that map to our data of interest, we need to get the data into an addressable form.</p>
<p>We need to understand how to load and understand CT scan data, which is usually stored in a DICOM file format. The MetaIO format is suggested for easier use, and the Python SimpleITK library can be used to convert it to a NumPy array. Each CT scan is uniquely identified by a series instance UID. The Hounsfield Unit (HU) scale is used to measure CT scan voxel density, with air at -1000 HU, water at 0 HU, and bone at least 1000 HU.</p>
<p>It's important to clean the data by removing outlier values to prevent difficulties in modeling.</p>
<h1 id="data-ranges-and-model-inputs"><a class="header" href="#data-ranges-and-model-inputs">Data Ranges and Model Inputs</a></h1>
<p>[...] We add channels of information to our samples. To prevent overshadowing of the new channels by raw HU values, we must be aware that our data ranges from <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">000</span></span></span></span> to <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8389em;vertical-align:-0.1944em;"></span><span class="mord">+</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">000</span></span></span></span>. We won't add more channels of data for the classification step, so our data handling will remain the same.</p>
<p>[modified]
For deep learning models, fixed-size inputs are necessary due to a fixed number of input neurons. Therefore, we need to produce a fixed-size array containing the candidate in order to use it as input to our classifier. We want to train our model using a crop of the CT scan that accurately centers the candidate, making identification easier for the model by decreasing the variation in expected inputs.</p>
<h2 id="the-patient-coordinate-system"><a class="header" href="#the-patient-coordinate-system">The Patient Coordinate System</a></h2>
<p>The candidate center data expressed in millimeters, not voxels. We need to convert our coordinates from the millimeter-based coordinate system <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="mclose">)</span></span></span></span> to the voxel-address-based coordinate system <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mclose">)</span></span></span></span>. The patient coordinate system defines the positive <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span> to be patient left, positive <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span></span> to be patient behind, and the positive <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span></span></span></span> to be toward patient head. The patient coordinate system is often used to specify the locations of interesting anatomy in a way that is independent of any particular scan.</p>
<h2 id="ct-scan-shape-and-voxel-sizes"><a class="header" href="#ct-scan-shape-and-voxel-sizes">CT Scan Shape and Voxel Sizes</a></h2>
<p>[???]
The size of the voxels varies between CT scans and typically are not cubes. The row and column dimensions usually have voxel sizes that are equal, and the index dimension has a larger value, but other ratios can exist. Understanding these details can help in interpreting the results visually.</p>
<h2 id="converting-between-millimeters-and-voxel-addresses"><a class="header" href="#converting-between-millimeters-and-voxel-addresses">Converting Between Millimeters and Voxel Addresses</a></h2>
<p>Converting between patient coordinates in millimeters and (I,R,C) array coordinates, we define some utility code to assist with the conversion. Flipping the axes is encoded in a <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span></span></span></span> matrix returned as a tuple from the ct_mhd. The metadata we need to convert from patient coordinates to array coordinates is contained in the MetaIO file alongside the CT data itself.</p>
<p>In CT scan images of patients with lung nodules, most of the data is not relevant to the nodule (up to 99.9999%). To extract the nods, an area around each candidate will be extracted, so the model can focus on one candidate at a time.</p>
<p>The implementation involves building a dataset by subclassing PyTorch Dataset. The LunaDataset class flattens a CT's nodules into a single collection. The implementation of this class requires two methods: firdt method returns the number of samples in the dataset, whereas the second returns a sample data needed to train (or validate).</p>
<h2 id="constructing-our-dataset"><a class="header" href="#constructing-our-dataset">Constructing our dataset</a></h2>
<h2 id="trainingvalidation-split"><a class="header" href="#trainingvalidation-split">Training/validation split</a></h2>
<p>We are going to divide the samples into a training set and a validation set in LunaDataset.</p>
<p>We can create two <code>Dataset</code> instances to ensure there is strict segregation between our training data and our validation data, depending on the task at hand. we should ensures a consistent sorted order, which helps with the segregation.</p>
<p>The training set should not provide unfair hints about the validation set that wouldn't be true for real-world data.</p>
<h1 id="rendering-the-data"><a class="header" href="#rendering-the-data">Rendering the data</a></h1>
<p>To render the data pertaining to CT and nodule slices. The data rendering process helps in getting an intuitive sense of what the data inputs look like. It also facilitates issue investigation by quickly identifying unusual samples or those that have problems. Effective data rendering is helpful in gaining familiarity with the data and in modifying it to tackle complex projects.</p>
<h1 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h1>
<p>This chapter helped us transform raw data into tensors using PyTorch. These design decisions including input size, caching structure and partitioning of training and validation sets have a significant impact on the success or failure of our overall project. Therefore, revisiting these decisions later on while working on our projects is advised. We are now set to implement a model and a training loop in the next chapter.</p>
<h1 id="initializing-the-model-and-optimizer"><a class="header" href="#initializing-the-model-and-optimizer">Initializing the model and optimizer</a></h1>
<p>In this section, the code initializes a model (named LunaModel) and an optimizer. The internal workings of the model are not discussed until section 11.4. The details of the optimizer used are explained, and the significance of self.model.to(device) is highlighted in ensuring the optimizer uses the copied GPU-based parameter objects rather than leaving the optimizer looking at the CPU-based parameter objects.</p>
<h1 id="dataparallel-vs-distributeddataparallel"><a class="header" href="#dataparallel-vs-distributeddataparallel">DataParallel vs. DistributedDataParallel</a></h1>
<p>Working with multiple GPUs can be handled using DataParallel or DistributedDataParallel. This book uses DataParallel because it's a simple wrapper that is almost entirely transparent in terms of the model implementation and the code using that model. DistributedDataParallel is a more complex wrapper, and the setup and configuration can be nontrivial. However, PyTorch provides it as the recommended wrapper class for multi-GPU or multi-machine use cases.</p>
<h1 id="care-and-feeding-of-data-loaders"><a class="header" href="#care-and-feeding-of-data-loaders">Care and feeding of data loaders</a></h1>
<p>The PyTorch DataLoader class helps with collating sample tuples into a batch tuple, allowing multiple samples to be processed at the same time. DataLoader handles all of the collation work and can provide parallel loading of data by using separate processes and shared memory. The collated data are used to train the model. Working with a validation set is also discussed, where validation_ds and validation_dl instances are similar, except for the obvious isValSet_bool=True.</p>
<h1 id="our-first-pass-neural-network-design"><a class="header" href="#our-first-pass-neural-network-design">Our first-pass neural network design</a></h1>
<p>The text concludes with a reference to the first-pass neural network design to be designed in the coming chapters. Discussions continue on how to adjust the project configuration settings systematically in a hyperparameter search. The section ends with advice on using data-loading features to increase the speed of projects as loading and processing data can overlap with GPU calculation.
In this text, the author discusses the design of a convolutional neural network for detecting tumors. They state that although the design space for such a network is vast, there have been effective models for image recognition that can be used as a starting point. A pre-existing network design will be modified for the project, with some adjustments made due to the input data being 3D. The text includes an image of the overall structure of the network and mentions that the four repeated blocks that make up most of the network will be examined in more detail. The author believes that this project will provide a good foundation for future projects, although some adaptation may be necessary depending on the specific project.
Convolutional neural networks typically have a tail, backbone, and head. The tail processes the input, while the backbone contains most of the layers arranged in series of blocks. The head converts the output from the backbone to the desired output form. One block consists of two 3x3 convolutions followed by max-pooling. Stacking convolutional layers allows the final output to be influenced by input beyond the size of the convolutional kernel. A fully connected layer followed by nn.Softmax makes up the tail. Softmax is used for single-label classification tasks and expresses the degree of certainty in an answer.
The text discusses the implementation of a deep learning model called Luna, used for computer-aided detection of lung nodules in medical images. The model uses convolutional neural networks and softmax layers to classify the images. The text also covers techniques for initializing the model parameters and the training process for the model. Different from previous training loop examples, the author uses a tensor to collect per-class metrics while iterating over the train data loader, and the actual loss computation is done in the computeBatchLoss method. The purpose of the <code>trnmetrics_g</code> tensor is to store information about the model's behavior on a per-sample basis from the <code>computeBatchLoss</code> function to the <code>logMetrics</code> function.
The <code>computeBatchLoss</code> function calculates the loss over a batch of samples which is used by both the training and validation loops. The core functionality of the function is feeding the batch into the model and computing the per-batch loss. By recording the label, prediction, and loss for each sample, we can have a wealth of detailed information we can use to investigate the behavior of our model. The validation loop is similar to the training loop, but without updating network weights. Per epoch, the performance metrics are logged and the progress is tracked. This logging is important because it helps us to notice when training is going off track and to keep an eye on how our model behaves.</p>
<h2 id="logmetrics-function"><a class="header" href="#logmetrics-function"><code>logMetrics</code> Function</a></h2>
<p>The <code>logMetrics</code> function displays the results of the <code>computeBatchLoss</code> function with details about the training or validation samples. The <code>mode_str</code> argument indicates whether the metrics are for training or validation.</p>
<p>Two input tensors <code>trnmetrics_t</code> and <code>valMetrics_t</code> are used to log the results. Both tensors have floating-point values filled with the data during <code>computeBatchLoss</code>.</p>
<p>The function applies tensor masking and Boolean indexing while constructing masks. The purpose is to limit the metrics to only the nodule or non-nodule samples and count the total samples per class, as well as the number of samples that are classified correctly.</p>
<p>The function then computes some per-label statistics and stores them in a dictionary, <code>metrics_dict</code>. It determines the fraction of samples that are correctly classified, as well as the fraction that is correct from each label.</p>
<p>Finally, the results are displayed as percentages using the <code>log.info</code> function.</p>
<p>Running the script
To run the entire script, run:</p>
<pre><code class="language-bash">$ python -m p2ch11.training
</code></pre>
<p>This command is for Linux/Bash. Windows users might need to invoke Python Starting LunaTrainingApp differently, depending on the install method used.</p>
<h3 id="epoch-training"><a class="header" href="#epoch-training">Epoch Training</a></h3>
<p>The text talks about the first epoch of a deep learning model's training process. The first epoch is divided into 20,193 steps called batches, each containing 256 data points. The training progress is represented in a log format, showing the number of batches that have been completed and the current status of the training process.</p>
<p>The text also highlights the importance of preparing the data cache for training, which can take a significant amount of time to process. The exercises in Chapter 10 provide tools and scripts to make the caching process more efficient. Once the data is loaded and the training process begins, monitoring the performance of the computing resources is crucial to ensure that resources are being used effectively.</p>
<p>The text also discusses the need to maximize the use of available computing resources during the training process to minimize the training time. The authors recommend checking the CPU and GPU load to identify the root of performance bottlenecks that may arise during the training process. It is suggested that these bottlenecks can be addressed by optimizing the caching process and using more efficient waiting strategies during the training process.
The text discusses data requirements for deep learning training and the need for sanity checks to ensure all data is accounted for. The <code>enumerateWithEstimate</code> function is introduced as a tool for estimating completion times during training. The output of the model training script is analyzed, showing the need to consider consequences of misclassification. The text also introduces TensorBoard as a tool for graphing training metrics and mentions its official support in PyTorch.
To use TensorBoard, we need to install tensorflow. We can install the default CPU-only package. We also need to segregate our data into separate folders for each project to make it easier to manage as TensorBoard can get quite complex. Once installed, we can start TensorBoard by invoking it from any directory and pointing it to the directory where our data is stored. After starting TensorBoard, we should be able to access the main dashboard by pointing our browser to http://localhost:6006. The main part of the screen displays training and validation metrics in a graphical format, making it easier to interpret the data. We can adjust the smoothing option to remove noise from trend lines if our data is noisy. We can also select which runs to display and delete runs that are no longer of interest. To add our data to TensorBoard, we use the <code>torch.utils.tensorboard</code> module to write metrics data in a format that TensorBoard can understand. We create <code>SummaryWriter</code> objects for the training and validation runs and write the data for each epoch using the <code>add_scalar</code> method. We can also add comments to our training script to make the data more informative. Finally, we can use TensorBoard to visualize our data and make it easier to analyze.
To keep your <code>runs/</code> directory clean, it's important to delete the runs that didn't yield useful results. Writing scalars is easy. We can use the <code>metrics_dict</code> we've constructed and pass each key/value pair to the <code>writer.add_scalar</code> method. This method is found in the <code>torch.utils.tensorboard.SummaryWriter</code> class with the signature <code>add_scalar(tag, scalar_value, global_step=None, walltime=None)</code>.</p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<p>In the given code snippet for adding values to TensorBoard graphs, the <code>tag</code> parameter gives the name of the graph, the <code>scalar_value</code> represents the Y-axis value, and the <code>global_step</code> parameter indicates the X-axis value. In the <code>doTraining</code> function, <code>totalTrainingSamples_count</code> is used as X-axis by providing it as input to the <code>global_step</code> parameter.</p>
<p>The key names separated by slashes create groups of charts with the substring before the '/'. Although the documentation advises using the epoch number as the <code>global_step</code> parameter, using the count of training samples that were presented to the network could be more beneficial, particularly when fewer samples are present or the number of samples is subject to change.</p>
<p>The loss trend lines highlight that the model is learning something. However, the model struggles to learn correctly about the desired output, particularly because the vast majority of cancer-detection answer set (around 99.7%) is false. The model typically decides that the answer to every question is false, which is similar to a student merely marking all the answers false in a test. However, the grades do not only reflect real knowledge, such as obtaining a comprehensive understanding of the topic by getting more questions right. Hence, to enhance the output, the author suggests introducing meaningful terms and a better way of grading.</p>
<h1 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h1>
<p>The chapter has provided a model and a training loop, and now we are able to use the data produced in the previous chapter. The metrics are being displayed in the console and graphed visually, but the results are not fully usable yet. However, the upcoming chapter will focus on improving the metrics and using them to make necessary changes in order to achieve better results.</p>
<h2 id="exercises"><a class="header" href="#exercises">Exercises</a></h2>
<ol>
<li>
<p>Develop a program which measures how long it takes to iterate through a LunaDataset instance by wrapping it in a DataLoader instance. Then, compare the performance of this program to the ones in chapter 10. During the program execution, it is important to consider the state of the cache.</p>
<p>a. What would be the effects of setting <code>num_workers</code> to 0, 1 or 2?</p>
<p>b. What is the highest value of <code>batch_size</code> and <code>num_workers</code> that your machine will support without running out of memory?</p>
</li>
<li>
<p>Reverse the order of <code>noduleInfo_list</code>. What is the impact on the model after one epoch of training?</p>
</li>
<li>
<p>Modify <code>logmetrics</code> to adjust the naming scheme of the runs and keys utilized in TensorBoard.</p>
<p>a. Experiment with different forward-slash placements for keys passed into <code>writer.add_scalar</code>.</p>
<p>b. Use the same writer for both training and validation runs, and include the string &quot;trn&quot; or &quot;val&quot; in the key name.</p>
<p>c. Customize the directory's naming and the keys to your preference.
Data loaders can be used to load data from various sets by utilizing various processors. This enables unused CPU resources to be utilized for preparing data to be fed to the GPU.</p>
</li>
</ol>
<p>Each dataset may contain several samples which can be loaded by data loaders in batches. PyTorch model processing is designed to operate on batches of data and not individual samples.</p>
<p>Data loaders can be used to modify data by adjusting the frequency of specific samples. This can be done to enhance or modify the dataset, although it may be more rational to directly modify the dataset.</p>
<p>In part 2, PyTorch's torch.optim.SGD optimizer will be utilized with a learning rate of 0.001 and a momentum of 0.99, which are the default values for most deep learning projects.</p>
<p>The initial model employed for classification will be quite similar to the model used in chapter 8 in order to start with a model that is believed to be effective. If it hinders the project's performance, it can be revisited.</p>
<p>For the majority of the deep learning projects, it's critical to choose appropriate metrics while monitoring the training sessions. By using misleading metrics, it is possible that the overall accuracy may not be as expected. Chapter 12 will explain how appropriate metrics can be chosen while evaluating.</p>
<p>TensorBoard can be utilized to represent numerous metrics visually, making it easier to consume such data as it varies per epoch of training.</p>
<p>This chapter focuses on how to quantify, express, and then enhance the performance of our model. We will adopt a metaphor derived from the &quot;Guard Dogs and Birds and Burglars&quot; approach to make the chapter's concepts more relatable. We will then create a visual language to represent some of the principal concepts needed to formally discuss the shortcomings of the implementation discussed in the previous chapter. Ratios such as Recall and Precision will be dealt with, and we will devise a way to score our model's performance, encapsulated in a single number as a New Metric, F1 Score. Finally, we'll introduce changes to LunaDataset to improve our training results, which include Balancing and Augmentation.</p>
<p>Our goal is to improve the performance of our trained model. By the end of the chapter, the model will perform much better and be capable of generating results that are clearly superior to chance. False positives and false negatives will be more explicitly discussed as well.</p>
<p>Roxie and Preston are two well-intentioned guard dogs from obedience school. While both dogs bark at burglars, Roxie barks at almost anything, while Preston only barks occasionally. Roxie makes too many false positive alerts, such as to thunderstorms and fire engines, so we will focus on the topic of false positives and false negatives in the chapter.
The text discusses the issues with using guard dogs to protect a house, as they can have a high number of false negatives, meaning they may ignore a real threat. The author uses a visual representation to explain true and false positives/negatives, in which burglars and rodents are considered threats, while birds are not. The X-axis shows the bark-worthiness of each event as determined by the guard dogs, while the Y-axis shows properties that humans perceive but dogs cannot. The model used to protect against cancer, which is much more complex than guarding a house, maps events and properties to a two-dimensional space so positive and negative events can be separated. The quadrant areas can be used to evaluate how well the model performs.</p>
<h2 id="recall-and-precision-in-guard-dogs"><a class="header" href="#recall-and-precision-in-guard-dogs">Recall and Precision in Guard Dogs</a></h2>
<p>Recall is the ability to identify all relevant things, while precision is the ability to identify only relevant things. In guard dog terms, recall means never missing any potential robbers, while precision means only barking at burglars.</p>
<p>To improve recall, minimize false negatives by barking at anything that could potentially be a robber. This means pushing the classification threshold all the way to the left to encompass nearly all positive events. Roxie the dog has an incredibly high recall due to her barking at everything, but this leads to a large number of false positives.</p>
<p>To improve precision, minimize false positives by only barking at certain things. This means pushing the classification threshold all the way to the right to exclude a large number of negative events. Preston the dog has an incredibly high precision due to his only barking at burglar behavior, but this leads to a large number of false negatives.</p>
<p>While neither precision nor recall can be the single metric used to grade a dog's performance, they are both useful numbers to have on hand during training. It is important to balance both recall and precision when training a guard dog to identify potential robbers.
Implementing Precision and Recall in logMetrics</p>
<p>The metrics precision and recall are important during training because they reveal how well models are performing. If either of these metrics drops to zero, it could mean the model is behaving poorly. We would like to update the logmetrics function to include precision and recall in its output, so we can monitor them alongside the loss and correctness metrics. We have already calculated some of the values we need, but we have to include false positive and false negative values for the rest. With these values, we can compute precision and recall and store them in metrics-dict.</p>
<p>The ultimate performance metric is the F1 score, which combines the values of precision and recall. The F1 score is better than averaging since averaging assigns the same score of 0.5 to values of 1.0 and 0.0, which is not meaningful. The F1 score implies a balance between precision and recall. We can use other metrics like <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">min</span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mclose">)</span></span></span></span>, but they penalize the imbalance between precision and recall and do not capture the nuance between the values. Finally, we could multiply precision and recall's values, but this is not beneficial when the results are close to perfect.
The text discusses the importance of having a metric that's sensitive to changes in the early stages of model design, and therefore, the author opted to use the F1 score to evaluate classification model performance. The author updates the logging output to include precision, recall, and F1 score by including the exact values for the count of correctly identified and the total number of samples for both negative and positive samples. The new metrics result in drastically poorer performance results for the model. The author mentions that having an ideal dataset entails balancing positive and negative samples to better train the model. Currently, the dataset is imbalanced, with a 400:1 ratio of positive samples to negative ones, which is making &quot;actually nodule&quot; samples get lost in the crowd. As there are too few positive samples among the training data, the author proposes changing the class balance of the training data to look more like an &quot;ideal&quot; dataset.
The article discusses the importance of balancing training data for building and training machine learning models. The imbalanced data can lead to the degenerate behavior of the model scoring well by answering only one label, which is not useful in the real-world scenario. To achieve discrimination, the dataset needs to be updated to alternate between positive and negative samples in a balanced manner. One way to accomplish this is by using samplers that reshape, limit, or reemphasize the underlying data. However, in this article, the implementation of class balancing within the dataset is discussed where the positive and negative training samples are kept separate and alternated to prevent the degenerate behavior of the model. The article also raises concerns regarding the real-world discriminatory bias being present in the models trained from the internet-at-large data sources.
The text discusses the importance of using a balanced dataset for training a neural network model to improve its performance in predicting positive samples. It outlines a method of creating a dataset with a 2:1 ratio of negative to positive samples and shows how to implement it in the code. The article also describes the process of training the model using TensorBoard and highlights the problem of overfitting, which occurs when the training loss improves while the validation loss deteriorates. The article ends by emphasizing the need to stop the training process if overfitting occurs to prevent the model from getting worse in real-world scenarios.</p>
<h1 id="revisiting-the-problem-of-overfitting"><a class="header" href="#revisiting-the-problem-of-overfitting">Revisiting the problem of overfitting</a></h1>
<p>Overfitting occurs when a model learns specific properties of the training set, losing the ability to generalize, and making it less accurate in predicting samples that haven't been trained on. For instance, a model can memorize quirks of a small set of positive training samples and consider everything else negative, which decreases its generalization ability.</p>
<p>To avoid overfitting, we must examine the right metrics. Looking at our overall loss, everything might seem fine, but that's because our validation set is unbalanced, and the negative samples dominate, making it hard for the model to memorize individual details. We need to make our training set and validation set both trend in the right direction to achieve better results. Figure 12.19 shows that our negative loss looks great since we have more negative samples (400 times) and, thus, it's harder for the model to memorize individual details.</p>
<p>Although some generalization is still going on, since we are classifying about 70% of the positive validation set correctly, we must change our training approach to improve our model's ability to recognize the general properties of the classes we are interested in. Overfitting is a common situation in machine learning that requires our attention to ensure the model's accuracy and reliability.
The article discusses the concept of overfitting and how it can affect a model that predicts the age of human faces. An overfit model remembers specific individuals' identifying details instead of developing a general model based on age signifiers. This leads to inaccuracies in predicting the age of a new picture. To prevent overfitting, the article suggests data augmentation, which involves modifying a dataset by applying synthetic alterations to individual samples, resulting in a new dataset with a larger number of effective samples. Five specific data augmentation techniques are discussed, including mirroring the image, shifting it by a few voxels, scaling it up or down, rotating it around the head-foot axis, and adding noise to the image. The article also provides code snippets to help readers understand how to implement these techniques.</p>
<h1 id="data-augmentation-techniques-for-medical-imaging"><a class="header" href="#data-augmentation-techniques-for-medical-imaging">Data Augmentation Techniques for Medical Imaging</a></h1>
<p>This text discusses various data augmentation techniques that can be used to improve the accuracy of machine learning models when applied to medical image data. The techniques are designed to create new training samples from the existing ones by applying simple transformations. The transformations include shifting/mirroring, scaling, rotation, and adding noise. The article provides code examples to demonstrate the implementation of each augmentation technique that can be integrated into a machine learning pipeline. In addition, the article discusses how these techniques can be examined and compared to select the best augmentation strategy.</p>
<h2 id="summary-1"><a class="header" href="#summary-1">Summary:</a></h2>
<p>This chapter discusses how to evaluate a model's performance and the importance of understanding the factors that contribute to it. It also covers dealing with insufficiently populated data sources and synthesizing representative training samples. The focus then shifts to finding candidate nodules and classifying them as malignant or benign in the upcoming chapters.</p>
<h1 id="exercises-1"><a class="header" href="#exercises-1">Exercises</a></h1>
<h2 id="1-generalizing-f1-score"><a class="header" href="#1-generalizing-f1-score">1. Generalizing F1 score</a></h2>
<p>The F1 score can be extended to support values other than 1.</p>
<p>a) Click the link to https://en.wikipedia.org/wiki/F1_score and implement F2 and F0.5 scores.</p>
<p>b) Determine the most appropriate score (F1, F2 or F0.5) for the project, track the score and compare it with the F1 score.</p>
<h2 id="2-balancing-training-samples"><a class="header" href="#2-balancing-training-samples">2. Balancing training samples.</a></h2>
<p>Implement a WeightedRandomSampler approach to balance positive and negative training samples for the LunaDataset with a ratio_int value of 0.</p>
<p>a) How did you obtain the required information about the class of each sample?</p>
<p>b) Which of the two approached was easier, and which resulted in more readable code?</p>
<h2 id="3-experimenting-with-class-balancing-schemes"><a class="header" href="#3-experimenting-with-class-balancing-schemes">3. Experimenting with class-balancing schemes</a></h2>
<p>a) What ratio of class balance produces the best score after two and twenty epochs?</p>
<p>b) What if the ratio is a function of epoch_ndx?</p>
<h2 id="4-experimenting-with-data-augmentation"><a class="header" href="#4-experimenting-with-data-augmentation">4. Experimenting with data augmentation</a></h2>
<p>a) Can any of the existing augmentation techniques be made more aggressive?</p>
<p>b) Does including noise augmentation help or hinder the training results?</p>
<p>c) Research data augmentation in other projects. Are any of them applicable here? Implement &quot;mixup&quot; augmentation for positive nodule candidates. Does it help?</p>
<h2 id="5-retrain-model-with-custom-initial-normalization"><a class="header" href="#5-retrain-model-with-custom-initial-normalization">5. Retrain model with custom initial normalization</a></h2>
<p>a) Can better results be obtained by using fixed normalization?</p>
<p>b) What normalization offset and scale make sense?</p>
<p>c) Do nonlinear normalization techniques such as square roots help?</p>
<h2 id="6-displaying-other-data-on-tensorboard"><a class="header" href="#6-displaying-other-data-on-tensorboard">6. Displaying other data on Tensorboard</a></h2>
<p>a) Can TensorBoard display information about the weights of your network?</p>
<p>b) Can intermediate results from running your model on a specific sample be displayed on TensorBoard?</p>
<p>c) Does having the backbone of your model wrapped in an instance of nn.Sequential help or hinder this effort?</p>
<h2 id="7-bonus-question"><a class="header" href="#7-bonus-question">7. Bonus question</a></h2>
<p>Hint: It is not about the F1 score!
The text explains the concepts of precision, recall, and F1 score, which are metrics used to evaluate the performance of a classification model. It also presents strategies to improve the model's performance through balanced training sets and data augmentation.</p>
<p>The focus of the chapter is the process of segmentation to identify possible nodules, which is step 2 of the project's plan. The segmentation model is created using a U-Net and involves updating the model, dataset, and training loop. The objective is to flag voxels that might be part of a nodule and use the classification step to reduce the number of incorrectly marked voxels. The chapter explains the steps involved in creating a model for segmentation, including per-pixel labeling and training with masks. Finally, the results of the new model are evaluated through quantitative segmentation.</p>
<h1 id="various-types-of-segmentation"><a class="header" href="#various-types-of-segmentation">Various types of segmentation</a></h1>
<p>This article talks about different types of segmentation, specifically about semantic segmentation. Semantic segmentation classifies pixels in an image into labels such as &quot;cat&quot;, &quot;dog&quot;, etc. resulting in distinct regions identifying things like &quot;all of these pixels are part of a cat&quot;. The article also briefly discussed instance segmentation and object detection, which are more complicated approaches. However, for this project, they are not the best approaches to find nodule candidates.</p>
<h2 id="semantic-segmentation-per-pixel-classification"><a class="header" href="#semantic-segmentation-per-pixel-classification">Semantic segmentation: Per-pixel classification</a></h2>
<p>Semantic segmentation identifies different objects and where they are in a given image. If there are multiple cats in an image, semantic segmentation can identify each cat's position. The existing classification models can't pinpoint where the cat is; they can only predict whether or not a cat is present in the image.</p>
<p>Semantic segmentation requires combining raw pixels to develop specific detectors for items like color and then building on this to create more informative feature detectors to finally identify specific things like a cat or a dog. Nonetheless, the segmentation model will not give us a single classification-like list of binary flags like classification models since the output should be a heatmap or mask.</p>
<p>If we use only convolutional &quot;layers&quot; without downsampling, we can get an output the same size as the input, but our receptive field will be very limited. Meaning that each segmented pixel will only consider a very localized area.
Assuming <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span></span></span></span> convolutions, the size of the receptive field for a simple model of stacked convolutions is <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>, with L being the number of convolutional layers. For instance, four layers of <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span></span></span></span> convolutions will have a receptive field size of <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">9</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">9</span></span></span></span> per output pixel. By inserting a <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">2</span></span></span></span> max pool between the second and third convolutions, and another at the end, we increase the receptive field to <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">16</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">16</span></span></span></span>. However, this happens after the first max pool, which makes the final effective receptive field <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">12</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">12</span></span></span></span> in the original input resolution. One common way to improve the receptive field of an output pixel while maintaining a 1:1 ratio of input pixels to output pixels is to use a technique called upsampling.</p>
<p>The U-Net architecture is a design for a neural network that can produce pixelwise output and was invented for segmentation. The design of this architecture is complicated, as it is a lot different compared to the mostly sequential structure of the classifiers. The U-Net architecture is good for image segmentation. The model has a U-shape, and it operates at different resolutions. It first goes from top left to bottom center through a series of convolutions and downscaling, then uses upscaling convolutions to get back to the full resolution. The U-Net author added the skip connections in the model to address the previous design problems, which skip connections are short-circuited inputs along the downsampling path into the corresponding layers in the upsampling path. The key innovation behind U-Net is having the final detail layers operating with the best of both worlds.</p>
<p>To perform segmentation, we need a model that is capable of outputting a probability for every pixel. Instead of implementing a custom U-Net segmentation model from scratch, we are going to update an existing implementation from an open source repository on GitHub.
It is important to understand the license terms of open source software used in a project, even if it is for personal use. The MIT license, although flexible, still has requirements. The author still has copyright to their work even if they publish it on platforms like GitHub. It is not in the public domain unless explicitly stated.</p>
<p>To understand a model's architecture, it is suggested to inspect the code and identify the building blocks. This exercise can aid in recognizing skip connections and creating a diagram for the model layout.</p>
<p>When searching for a suitable model implementation, it is recommended to keep an eye out for models that can be adapted to fit the project's needs. It is essential to familiarize oneself with models that exist and their implementations and training processes. Also, knowledge of what parts can be scavenged and applied to current projects can be helpful. Building this knowledge toolkit is important, even for beginners.</p>
<h2 id="adapting-an-off-the-shelf-model-to-our-project"><a class="header" href="#adapting-an-off-the-shelf-model-to-our-project">Adapting an off-the-shelf model to our project</a></h2>
<p>We're going to make changes to the classic U-Net model, to better suit our project's needs. We'll pass the input through batch normalization to get normalization statistics and restrict output values in the range <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span> with a sigmoid layer. We will also reduce the model's depth and number of filters. Our output will be in a single channel. We can wrap U-Net by implementing a model with three attributes.</p>
<h2 id="updating-the-dataset-for-segmentation"><a class="header" href="#updating-the-dataset-for-segmentation">Updating the dataset for segmentation</a></h2>
<p>Our source data remains the same: CT scans and annotation data. But, our model produces output of a different form than we had previously. To solve this, we need to produce 2D data now. We'll use padded convolutions and certain input sizes for the U-Net model to get the output of the same size.</p>
<p>U-Net has specific input size requirements, causing some regions near the edges of the image to be artificially padded, but that's a compromise we'll accept.</p>
<h2 id="u-net-trade-offs-for-3d-vs-2d-data"><a class="header" href="#u-net-trade-offs-for-3d-vs-2d-data">U-Net trade-offs for 3D vs. 2D data</a></h2>
<p>The data we have is of 3D, doesn't line up exactly with the 2D expected input. Feeding a 512 x 512 x 128 image into a converted-to-3D U-Net class won't work due to the GPU memory exhaustion. For example, the first layer of U-net is 64 channels, which is <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">8</span><span class="mord mathnormal" style="margin-right:0.05017em;">GB</span></span></span></span> just for the first convolutional layer.</p>
<p>Instead of handling things in 3D, we treat each slice as a 2D segmentation problem and use neighbouring slices as separate channels. We lose the direct spatial relationship between slices as all channels are combined by the convolution kernels.</p>
<p>We also lose the wider receptive field in the depth dimension that would come from a true 3D segmentation with downsampling. In contrast, CT slices are usually thicker than the resolution in rows and columns.</p>
<p>There isn't an easy flowchart or rule of thumb that can give canned answers to questions about which trade-offs to make, or whether a given set of compromises compromise too much. Careful experimentation is key, and systematically testing hypothesis after hypothesis can help narrow down which changes and approaches are working well for the problem at hand.</p>
<h2 id="building-the-ground-truth-data"><a class="header" href="#building-the-ground-truth-data">Building the ground truth data</a></h2>
<p>We have annotated points but we want a per-voxel mask that indicates whether any given voxel is part of a nodule. We'll have to build that mask ourselves from the data we have and then do manual checking to make sure the routine that builds the mask is performing well.</p>
<p>Our API allows us to easily grab results and plot them in a notebook. We are going to begin by converting the nodule locations we have into bounding boxes that cover the entire nodule.</p>
<p>To accomplish this goal, we start the origin of our search at the annotated center of our nodule. We then examine the density of the voxels adjacent to our origin on the column and row axis until we hit low-density voxels, which indicate that we've reached normal lung tissue. Finally, we search in the third dimension.</p>
<p>Our final bounding box is five voxels wide and seven voxels tall.
The text discusses how to create a bounding box around nodules in CT scans. This involves tracing outward from the nodule location in all three dimensions until low-density voxels are found. The bounding box is represented as a Boolean tensor which marks the voxels above the density threshold. The boundary of the box might include a portion of the lung wall, but this issue is not fixed. The text also mentions that the annotation data needs to be cleaned up, as several candidates are listed multiple times in the CSV file.
The text describes how to update the <code>getCandidateInfoList</code> function to pull nodules from a new annotations file. The data produced will be two-dimensional CT scans with multiple channels. These channels will hold adjacent slices of CT. The input to the segmentation model treats each slice as a single channel and produces a multichannel 2D image. The validation set will consist of one sample per slice of CT that has an entry in the positive mask. For handling the flag indicating whether this is meant to be a training or validation set, we partition the list of series into training and validation sets. The handling for the validation set needs to change. During training, we limit ourselves to only the CT slices that have a positive mask present. We need a new function that caches the size of each CT scan and its positive mask to disk. This is important to quickly construct the full size of a validation set without having to load each CT at Dataset initialization.</p>
<h2 id="summary-2"><a class="header" href="#summary-2">Summary</a></h2>
<p>This text describes the process of designing and creating datasets for training and validating a segmentation model for detecting nodules in CT scans. The <code>dsets.py</code> file contains the code for creating these datasets.</p>
<p>To create the datasets, the series_uid values representing the CT scans are identified and used to filter the <code>candidateInfo_list</code> to include only nodule candidates with a series_uid that is included in that set of series. Additionally, another list containing only the positive candidates is created for training purposes.</p>
<p>The <code>_getitem_</code> function is used to return the appropriate type (full slice or training crop) based on whether it's training or validation.</p>
<p>For training, instead of the full CT slices, <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">64</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">64</span></span></span></span> crops around the positive candidates are used. These crops are randomly taken from a <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">96</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">96</span></span></span></span> crop centered on the nodule. Three slices of context are also included as additional &quot;channels&quot; to the 2D segmentation.</p>
<p>The validation dataset uses the same convolutions with the same weights, but applied to a larger set of pixels. Due to the inclusion of more negative pixels, the model will have a high false positive rate during validation.</p>
<h1 id="implementing-trainingluna2dsegmentationdataset"><a class="header" href="#implementing-trainingluna2dsegmentationdataset">Implementing TrainingLuna2dSegmentationDataset</a></h1>
<p>This section of the text presents the code implementation of the &quot;TrainingLuna2dSegmentationDataset&quot;. The code snippet shows the implementation for a method named &quot;_getitem_&quot; for the training set, where samples are taken from &quot;pos_list&quot;. This method is almost the same as the one for the validation set. However, there is a difference, as the &quot;getItem_trainingCrop&quot; method is used to process the sample with candidate info from the tuple. This tuple includes both the series and the exact center location, which is not available in just the slice. The code snippet shared below provides additional details on the implementation of the &quot;_getitem_&quot; method for the &quot;TrainingLuna2dSegmentationDataset&quot;.</p>
<pre><code class="language-python"># Listing 13.17 dsets.py:320, __getitem__
def getitem__(self, ndx):
    candidateInfo_tup = self.pos_list[ndx % len(self.pos_list)]
    return self.getitem_trainingCrop(candidateInfo_tup)
</code></pre>
<p>The text discusses implementing <code>getitem_trainingCrop</code> using <code>getctRawCandidate</code> function, which crops the image to a smaller size and returns an array with an additional crop. The data augmentation process is moved to the GPU to avoid bottleneck issues. A new model is introduced called <code>SegmentationAugmentation</code>, which consumes and produces tensors similarly to other models. The <code>training.py</code> script is updated to instantiate the new model, introduce Dice loss, and logs more metrics such as TensorBoard images, along with saving based on validation. The new model's initialization is similar to that of <code>UNetWrapper</code>.
UNet Input and Output</p>
<ul>
<li>For input into UNet, we have seven input channels; <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">3</span></span></span></span> context slices, and 1 slice that is the focus for what we're actually segmenting.</li>
<li>We have one output class indicating whether this voxel is part of a nodule.</li>
</ul>
<p>UNet Depth and Filters</p>
<ul>
<li>The depth parameter controls how deep the U goes; each downsampling operation adds 1 to the depth.</li>
<li>The first layer will have 2 ** \wf==32 filters (using \wf=5), which doubles with each downsampling.</li>
</ul>
<p>UNet Convolution, Batch Normalization, and Upsampling</p>
<ul>
<li>We want the convolutions to be padded so that we get an output image the same size as our input.</li>
<li>We also want batch normalization inside the network after each activation function, and our upsampling function should be an upconvolution layer.</li>
</ul>
<p>Adam optimizer</p>
<ul>
<li>Adam maintains a separate learning rate for each parameter and automatically updates that learning rate as training progresses.</li>
<li>It's generally accepted that Adam is a reasonable optimizer to start most projects with.</li>
</ul>
<p>Dice Loss</p>
<ul>
<li>The Sørensen-Dice coefficient or Dice loss is a common loss metric for segmentation tasks.</li>
<li>It handles the case where only a small portion of the overall image is flagged as positive.</li>
<li>We're basically going to be using a per-pixel F1 score where the &quot;population&quot; is one image's pixels.</li>
<li>We're going to take our ratio and subtract it from 1 to invert the slope of our loss function so that in the high-overlap case, our loss is low; and in the low-overlap case, it's high.</li>
<li>We're going to update our computeBatchLoss function to call diceLoss.</li>
<li>By multiplying our predictions times the label (which are effectively Booleans), we'll get pseudo-predictions that got every negative pixel &quot;exactly right.&quot;</li>
<li>We're providing a loss that represents the fact of providing high recall.
The text discusses a method for optimizing a model for better recall in segmentation tasks by sacrificing many true negative pixels through a false negative loss function. The performance metrics are computed using <code>computeBatchLoss</code>, which uses per sample values and summary statistics for true positives and other metrics. The results are visually represented with <code>TensorBoard</code> through <code>logImages</code>, which displays ground truth and model outputs for selected CT scans. The output images can be compared with a slider that shows previous epochs' images.
The code creates an empty image with 512x512 pixels and three color channels. The image is used for flagging false positives and false negatives in a prediction. False positives are marked in red and overlaid on the image, while false negatives are marked in green. Pixels that are both false negatives and false positives are marked in orange. The final image is clipped between 0 and 1.
The goal is to have a grayscale CT image with predicted-nodule pixels in various colors. Red is used for incorrect pixels (false positives and false negatives), green is used for correctly predicted pixels inside the nodule, and half-strength mask added green is used for false negatives, which appear as orange. The data is then renormalized to the 0.1 range and saved to TensorBoard. Per-epoch metrics are computed, including true positives, false negatives, and false positives. The recall will be used to determine the best model for this training run, and a flag will be included to indicate the best score we've seen. Finally, the saveModel function is implemented to persist the model to disk.
In this section, we learn about how to save our model in PyTorch. It is recommended to save only the parameters of the model, using the model.state_dict() function. This approach allows us to load those parameters into any model that expects parameters of the same shape, even if the class doesn't match the saved model. We set the file path to our saved model. If we saved the optimizer state as well, we could resume training seamlessly. If the current model has the best score, we save a copy of state with a .best.state filename. PyTorch has the facility to save and load the model by using the torch.save and torch.load functions.</li>
</ul>
<p>We have made all our code changes, and now it’s time to check the results of our proposed models after implementation. We first look at training metrics, and then we compare them to validation metrics. The F1 score, TPs, FPs, and FNs are in rows we need to be concerned about since they should all trend towards improvement. We may have fewer FPs and FNs along with higher accuracy. The validation metrics in general look good, but the test of the model still needs to be done. We have a higher false positive rate, which is expected because of the larger size of pixels we validate.</p>
<p>Our recall plateaus and then starts dropping, indicating overfitting, which can be further confirmed from figure 13.18. U-Net architecture has great capacity, and even with our reduced filter and depth counts, it memorizes our training set fairly quickly.</p>
<h2 id="segmentation-recall-is-top-priority"><a class="header" href="#segmentation-recall-is-top-priority">Segmentation Recall is Top Priority</a></h2>
<p>In segmentation, recall is the top priority for the model, meaning the model should prioritize identifying all relevant instances, even if it results in some false positives. Precision can be handled downstream by classification models. As a result, it is more challenging to evaluate the model's performance, as it presents skewed results. The authors propose using the F2 score to mitigate this issue, but they will score the model solely based on recall and use human judgment to ensure that the training runs are not too focused on it. They trained their model on the Dice loss. However, this approach may not always be reliable, as educated guesses do not replace actual experimentation.</p>
<h2 id="good-enough-metrics"><a class="header" href="#good-enough-metrics">Good Enough Metrics</a></h2>
<p>The authors have already trained and evaluated the model for Chapter 14, so they know its outcome. However, there is no guarantee that the results will hold up in a new situation. Nevertheless, the model's performance is currently deemed good enough to proceed with the end-to-end project, despite some extreme metric values.
This chapter discusses a new model architecture called U-Net, which is used for pixel-to-pixel segmentation. The segmentation is different from classification as it flags individual pixels or voxels for membership in a class. We developed a training loop with the ability to save images to TensorBoard, and we have moved augmentation from the dataset into a separate model that can operate on the GPU. It is possible to train a segmentation model on image crops while validating on whole-image slices. In addition, we looked at our training results and adapted an implementation for our own use.</p>
<p>We can use segmentation followed by classification for detecting nodules. We avoid the potential leak from the training set to the validation set by splitting the data into a training set and an independent validation set. We must avoid this situation as it may lead to performance figures that would be artificially higher compared to what we would obtain on an independent dataset.</p>
<p>We should keep an eye on the end-to-end process when defining the validation set, and the easiest way to do this is by having two separate directories for training and validation. After the validation dataset is defined, we can train our model. We can bridge the CT segmentation and nodule candidate classification models to reach the goal of our project, which is to automatically detect cancer.</p>
<h1 id="chapter-14-writing-code-for-nodule-analysis"><a class="header" href="#chapter-14-writing-code-for-nodule-analysis">Chapter 14: Writing Code for Nodule Analysis</a></h1>
<p>The chapter focuses on grouping segmented voxel data into nodule candidates and further classifying those candidates using a malignancy classifier. The process uses a saved segmentation model from chapter 13 and a newly trained classification model.</p>
<p>The aim is to convert the segmentation output into sample tuples. The groupings work by finding the dashed outline around the highlight in figure 14.3. The input is the segmented voxels flagged by the segmentation model, and the output is the center of mass coordinates of each 'lump' of flagged voxels.</p>
<p>Running the models is similar to the training and validation processes, but with a loop over the CTs. For each CT, every slice is segmented, and the segmented output is used as the input to the grouping process. The grouping output is then used as input to a nodule classifier, and the surviving nodules are passed to a malignancy classifier.</p>
<p>The process is executed by an outer loop over the CTs, which segments, groups, classifies the candidates, and provides the classifications for further processing.</p>
<p>The methods- 'segmentct,' 'groupSegmentationoutput,' and 'classifyCandidates'- are broken down in the following sections.</p>
<p><img src="https://cdn.mathpix.com/cropped/2023_04_27_14ff7830b8aaf17904d3g-177.jpg?height=972&amp;width=1430&amp;top_left_y=189&amp;top_left_x=223" alt="figure 14.3" /></p>
<p><em>Note: Listing 14.1, i.e., <code>NoduleAnalysisApp.main</code>, is presented for further understanding.</em></p>
<h1 id="segmentation"><a class="header" href="#segmentation">Segmentation</a></h1>
<p>This step involves dividing the CT scan into individual slices. A <code>Dataset</code> is created, which loads a single slice of a CT scan with a given <code>series_uid</code>. The output of the segmentation step is an array of per-pixel probabilities, indicating whether the pixel is part of a nodule. These slice-wise predictions are collected in a mask array with the same shape as the CT scan input, and a threshold is applied to the predictions to obtain a binary array. A cleanup step is then performed using the <code>scipy.ndimage.morphology</code> erosion operation, which shortens the flagged area and removes small components. The code will use the GPU if it's available. The function used for this step is called <code>segmentct</code>.
The text discusses the process of identifying nodule candidates in CT scans for possible cancer detection. A connected-components algorithm is used for grouping the suspected nodule voxels. The labeled chunks are passed on to a classification module to reduce false positives. The text also discusses the difference between fully automated and assistive systems. The goal is to discard a large amount of irrelevant data from millions of data points to a handful of tumors. Finally, the identified regions in the CT scan are cropped and passed onto the classification module using DataLoader.
We use a data loader to loop over a candidate list to threshold the output probabilities to get a list of things our model thinks are actual nodules, which would be output for a radiologist to inspect while adjusting the threshold to err a bit on the safe side. A single CT scan from the validation set is run and 16 nodule candidates are found. A confusion matrix is created with our results, showing that we found the 1 malignant nodule but missed a 17th benign one, and 15 false positive non-nodules made it through the nodule classifier. Quantitative validation is done and we detected 132 of the 154 nodules, or 85%. Toward the end of this chapter, some pointers to papers and techniques that can help improve these numbers will be provided.</p>
<h1 id="predicting-malignancy"><a class="header" href="#predicting-malignancy">Predicting malignancy</a></h1>
<p>The article discusses the task of identifying malignant nodules from benign ones in CT scans after implementing the nodule-detection task of the LUNA challenge. Even with a good system, diagnosing malignancy would need a more comprehensive view of the patient, additional non-CT context, and a biopsy instead of just looking at a particular nodule in isolation on a CT scan. This task is likely to be performed by a doctor for some time to come.</p>
<p>The key takeaways from the article are:</p>
<ul>
<li>Splitting training and validation (and test) sets between patients is important to avoid errors.</li>
<li>Converting pixel-wise marks to nodules can be achieved using traditional image processing.</li>
<li>The diagnosis script performs both segmentation and classification. Fine-tuning can be used to modify a pretrained model while using a minimum of training data. We need to retrain a portion of the network-matching features to our task.</li>
<li>TensorBoard can help us visualise and identify network anomalies, but it is not a substitute for reviewing data that our model is not working well on.</li>
<li>Successful training involves an overfitting network stage which is then regularised.</li>
<li>There is no magic bullet when training neural networks.</li>
<li>Kaggle is an excellent source of project ideas for deep learning experimentation. Many new datasets have cash prizes for the top performers.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="pytorch1.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="pytorch1.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="./theme/tabbed-code-blocks.js"></script>
        <script src="./theme/highlight.css"></script>


    </div>
    </body>
</html>
